{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ICICI Bank Statement Analysis Demo\n",
        "\n",
        "This notebook demonstrates the complete workflow of processing an ICICI bank statement PDF, extracting transaction data, and comparing it with the expected results.\n",
        "\n",
        "## What we'll cover:\n",
        "1. **PDF Analysis** - Understanding the structure of the ICICI statement\n",
        "2. **Data Extraction** - Using pdfplumber to extract tables and text\n",
        "3. **Data Processing** - Converting raw data to structured format\n",
        "4. **Schema Validation** - Ensuring our output matches the expected CSV structure\n",
        "5. **Comparison** - Comparing extracted data with the reference `result.csv`\n",
        "\n",
        "## Files used:\n",
        "- `data/icici/icici sample.pdf` - Input bank statement\n",
        "- `data/icici/result.csv` - Expected output format\n",
        "- `custom_parsers/icici_parser.py` - Our custom parser implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options for better readability\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"\u2705 Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding the Expected Output Schema\n",
        "\n",
        "First, let's examine the `result.csv` file to understand the expected structure and data format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the expected result CSV\n",
        "result_csv_path = \"data/icici/result.csv\"\n",
        "expected_df = pd.read_csv(result_csv_path)\n",
        "\n",
        "print(\"\ud83d\udcca Expected Output Schema:\")\n",
        "print(f\"Shape: {expected_df.shape}\")\n",
        "print(f\"Columns: {list(expected_df.columns)}\")\n",
        "print(\"\\n\ud83d\udccb Data Types:\")\n",
        "print(expected_df.dtypes)\n",
        "print(\"\\n\ud83d\udd0d First few rows:\")\n",
        "display(expected_df.head())\n",
        "print(\"\\n\ud83d\udcc8 Summary statistics:\")\n",
        "display(expected_df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Analyzing the PDF Structure\n",
        "\n",
        "Now let's examine the ICICI sample PDF to understand its structure and identify where the transaction data is located."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open and analyze the PDF\n",
        "pdf_path = \"data/icici/icici sample.pdf\"\n",
        "\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    print(f\"\ud83d\udcc4 PDF Analysis Results:\")\n",
        "    print(f\"Total pages: {len(pdf.pages)}\")\n",
        "    \n",
        "    # Analyze each page\n",
        "    for page_num, page in enumerate(pdf.pages):\n",
        "        print(f\"\\n\ud83d\udcd6 Page {page_num + 1}:\")\n",
        "        \n",
        "        # Extract text\n",
        "        text = page.extract_text()\n",
        "        if text:\n",
        "            print(f\"Text length: {len(text)} characters\")\n",
        "            print(f\"First 200 chars: {text[:200]}...\")\n",
        "        \n",
        "        # Extract tables\n",
        "        tables = page.extract_tables()\n",
        "        print(f\"Tables found: {len(tables)}\")\n",
        "        \n",
        "        # Show table structures\n",
        "        for table_idx, table in enumerate(tables):\n",
        "            if table:\n",
        "                print(f\"  Table {table_idx + 1}: {len(table)} rows x {len(table[0]) if table[0] else 0} columns\")\n",
        "                if len(table) > 0:\n",
        "                    print(f\"    First row: {table[0]}\")\n",
        "                    if len(table) > 1:\n",
        "                        print(f\"    Second row: {table[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Extracting Transaction Data\n",
        "\n",
        "Let's extract the transaction data from the PDF tables and see what we get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract all tables from the PDF\n",
        "with pdfplumber.open(pdf_path) as pdf:\n",
        "    all_tables = []\n",
        "    \n",
        "    for page in pdf.pages:\n",
        "        tables = page.extract_tables()\n",
        "        all_tables.extend(tables)\n",
        "    \n",
        "    print(f\"\ud83d\udcca Total tables extracted: {len(all_tables)}\")\n",
        "    \n",
        "    # Display each table\n",
        "    for i, table in enumerate(all_tables):\n",
        "        if table and len(table) > 0:\n",
        "            print(f\"\\n\ud83d\udccb Table {i + 1}:\")\n",
        "            print(f\"Shape: {len(table)} rows x {len(table[0]) if table[0] else 0} columns\")\n",
        "            \n",
        "            # Convert to DataFrame for better display\n",
        "            df = pd.DataFrame(table)\n",
        "            if len(df) > 0:\n",
        "                display(df.head(10))  # Show first 10 rows\n",
        "                \n",
        "                # Check if this looks like a transaction table\n",
        "                if len(df.columns) >= 4:  # At least Date, Description, Debit, Credit columns\n",
        "                    print(f\"  \ud83c\udfaf Potential transaction table detected!\")\n",
        "                    print(f\"  \ud83d\udcc5 Columns: {list(df.iloc[0]) if len(df) > 0 else 'No data'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Implementing the Parser Logic\n",
        "\n",
        "Now let's implement the logic to extract and process transaction data, similar to what our agent would generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_transactions_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract transaction data from ICICI bank statement PDF.\n",
        "    This function implements the logic that our AI agent would generate.\n",
        "    \"\"\"\n",
        "    transactions = []\n",
        "    \n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            tables = page.extract_tables()\n",
        "            \n",
        "            for table in tables:\n",
        "                if not table or len(table) < 2:\n",
        "                    continue\n",
        "                    \n",
        "                # Look for transaction tables (should have multiple columns)\n",
        "                if len(table[0]) >= 4:\n",
        "                    # Check if first row looks like headers\n",
        "                    first_row = table[0]\n",
        "                    \n",
        "                    # Process each row (skip header)\n",
        "                    for row in table[1:]:\n",
        "                        if len(row) >= 4 and any(cell and str(cell).strip() for cell in row):\n",
        "                            # Extract transaction data\n",
        "                            transaction = {\n",
        "                                'Date': row[0] if len(row) > 0 else None,\n",
        "                                'Description': row[1] if len(row) > 1 else None,\n",
        "                                'Debit Amt': row[2] if len(row) > 2 else None,\n",
        "                                'Credit Amt': row[3] if len(row) > 3 else None,\n",
        "                                'Balance': row[4] if len(row) > 4 else None\n",
        "                            }\n",
        "                            \n",
        "                            # Clean up the data\n",
        "                            for key, value in transaction.items():\n",
        "                                if value is not None:\n",
        "                                    # Remove extra whitespace\n",
        "                                    if isinstance(value, str):\n",
        "                                        value = value.strip()\n",
        "                                    # Convert empty strings to None\n",
        "                                    if value == '':\n",
        "                                        value = None\n",
        "                                    transaction[key] = value\n",
        "                            \n",
        "                            # Only add if we have meaningful data\n",
        "                            if transaction['Date'] and transaction['Description']:\n",
        "                                transactions.append(transaction)\n",
        "    \n",
        "    return transactions\n",
        "\n",
        "# Extract transactions\n",
        "print(\"\ud83d\udd0d Extracting transactions from PDF...\")\n",
        "extracted_transactions = extract_transactions_from_pdf(pdf_path)\n",
        "print(f\"\u2705 Extracted {len(extracted_transactions)} transactions\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "extracted_df = pd.DataFrame(extracted_transactions)\n",
        "print(\"\\n\ud83d\udcca Extracted Data:\")\n",
        "display(extracted_df.head(10))\n",
        "print(f\"\\n\ud83d\udcc8 Shape: {extracted_df.shape}\")\n",
        "print(f\"\ud83d\udccb Columns: {list(extracted_df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Cleaning and Formatting\n",
        "\n",
        "Now let's clean and format the extracted data to match the expected schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_and_format_transactions(df):\n",
        "    \"\"\"\n",
        "    Clean and format the extracted transaction data to match expected schema.\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying original\n",
        "    cleaned_df = df.copy()\n",
        "    \n",
        "    # Convert numeric columns\n",
        "    numeric_columns = ['Debit Amt', 'Credit Amt', 'Balance']\n",
        "    \n",
        "    for col in numeric_columns:\n",
        "        if col in cleaned_df.columns:\n",
        "            # Remove currency symbols and commas\n",
        "            cleaned_df[col] = cleaned_df[col].astype(str).str.replace(r'[\u20b9,\u20b9\\s]', '', regex=True)\n",
        "            # Convert to numeric, errors='coerce' will set invalid values to NaN\n",
        "            cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')\n",
        "    \n",
        "    # Clean date column\n",
        "    if 'Date' in cleaned_df.columns:\n",
        "        # Remove any extra whitespace\n",
        "        cleaned_df['Date'] = cleaned_df['Date'].astype(str).str.strip()\n",
        "        # Convert empty strings to None\n",
        "        cleaned_df['Date'] = cleaned_df['Date'].replace('', None)\n",
        "    \n",
        "    # Clean description column\n",
        "    if 'Description' in cleaned_df.columns:\n",
        "        cleaned_df['Description'] = cleaned_df['Description'].astype(str).str.strip()\n",
        "        cleaned_df['Description'] = cleaned_df['Description'].replace('', None)\n",
        "    \n",
        "    # Remove rows where both Date and Description are None\n",
        "    cleaned_df = cleaned_df.dropna(subset=['Date', 'Description'], how='all')\n",
        "    \n",
        "    return cleaned_df\n",
        "\n",
        "# Clean the extracted data\n",
        "print(\"\ud83e\uddf9 Cleaning and formatting extracted data...\")\n",
        "cleaned_df = clean_and_format_transactions(extracted_df)\n",
        "\n",
        "print(\"\\n\ud83d\udcca Cleaned Data:\")\n",
        "display(cleaned_df.head(10))\n",
        "print(f\"\\n\ud83d\udcc8 Shape after cleaning: {cleaned_df.shape}\")\n",
        "print(\"\\n\ud83d\udd0d Data types:\")\n",
        "print(cleaned_df.dtypes)\n",
        "print(\"\\n\ud83d\udcca Summary statistics:\")\n",
        "display(cleaned_df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Schema Validation\n",
        "\n",
        "Let's validate that our extracted data matches the expected schema from `result.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_schema(extracted_df, expected_df):\n",
        "    \"\"\"\n",
        "    Validate that the extracted data matches the expected schema.\n",
        "    \"\"\"\n",
        "    print(\"\ud83d\udd0d Schema Validation Results:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check column names\n",
        "    expected_columns = list(expected_df.columns)\n",
        "    extracted_columns = list(extracted_df.columns)\n",
        "    \n",
        "    print(f\"\ud83d\udccb Expected columns: {expected_columns}\")\n",
        "    print(f\"\ud83d\udccb Extracted columns: {extracted_columns}\")\n",
        "    \n",
        "    # Check if all expected columns are present\n",
        "    missing_columns = set(expected_columns) - set(extracted_columns)\n",
        "    extra_columns = set(extracted_columns) - set(expected_columns)\n",
        "    \n",
        "    if missing_columns:\n",
        "        print(f\"\u274c Missing columns: {missing_columns}\")\n",
        "    else:\n",
        "        print(\"\u2705 All expected columns are present\")\n",
        "        \n",
        "    if extra_columns:\n",
        "        print(f\"\u26a0\ufe0f  Extra columns: {extra_columns}\")\n",
        "    else:\n",
        "        print(\"\u2705 No extra columns\")\n",
        "    \n",
        "    # Check data types\n",
        "    print(\"\\n\ud83d\udd0d Data Type Comparison:\")\n",
        "    for col in expected_columns:\n",
        "        if col in extracted_df.columns:\n",
        "            expected_type = expected_df[col].dtype\n",
        "            extracted_type = extracted_df[col].dtype\n",
        "            print(f\"  {col}: Expected {expected_type}, Got {extracted_type}\")\n",
        "        \n",
        "    # Check row count\n",
        "    print(f\"\\n\ud83d\udcca Row Count Comparison:\")\n",
        "    print(f\"  Expected: {len(expected_df)} rows\")\n",
        "    print(f\"  Extracted: {len(extracted_df)} rows\")\n",
        "    \n",
        "    return len(missing_columns) == 0\n",
        "\n",
        "# Validate schema\n",
        "schema_valid = validate_schema(cleaned_df, expected_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Data Comparison and Analysis\n",
        "\n",
        "Now let's compare our extracted data with the expected results and analyze any differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure both DataFrames have the same columns for comparison\n",
        "if schema_valid:\n",
        "    # Reorder extracted columns to match expected order\n",
        "    cleaned_df = cleaned_df[expected_df.columns]\n",
        "    \n",
        "    print(\"\ud83d\udcca Data Comparison:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Display both DataFrames side by side\n",
        "    print(\"\\n\ud83d\udccb Expected Data (result.csv):\")\n",
        "    display(expected_df.head())\n",
        "    \n",
        "    print(\"\\n\ud83d\udccb Extracted Data (from PDF):\")\n",
        "    display(cleaned_df.head())\n",
        "    \n",
        "    # Check for exact matches\n",
        "    print(\"\\n\ud83d\udd0d Checking for exact matches...\")\n",
        "    \n",
        "    # Reset indices for comparison\n",
        "    expected_reset = expected_df.reset_index(drop=True)\n",
        "    extracted_reset = cleaned_df.reset_index(drop=True)\n",
        "    \n",
        "    # Compare DataFrames\n",
        "    if expected_reset.equals(extracted_reset):\n",
        "        print(\"\u2705 Perfect match! Extracted data exactly matches expected results.\")\n",
        "    else:\n",
        "        print(\"\u26a0\ufe0f  Data doesn't match exactly. Let's analyze the differences...\")\n",
        "        \n",
        "        # Show differences\n",
        "        print(\"\\n\ud83d\udcca Shape comparison:\")\n",
        "        print(f\"Expected: {expected_reset.shape}\")\n",
        "        print(f\"Extracted: {extracted_reset.shape}\")\n",
        "        \n",
        "        # Compare first few rows in detail\n",
        "        print(\"\\n\ud83d\udd0d Detailed comparison of first few rows:\")\n",
        "        for i in range(min(3, len(expected_reset), len(extracted_reset))):\n",
        "            print(f\"\\nRow {i}:\")\n",
        "            print(f\"  Expected: {dict(expected_reset.iloc[i])}\")\n",
        "            print(f\"  Extracted: {dict(extracted_reset.iloc[i])}\")\n",
        "            \n",
        "            # Check if this row matches\n",
        "            if expected_reset.iloc[i].equals(extracted_reset.iloc[i]):\n",
        "                print(\"  \u2705 Row matches\")\n",
        "            else:\n",
        "                print(\"  \u274c Row doesn't match\")\n",
        "else:\n",
        "    print(\"\u274c Schema validation failed. Cannot compare data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visual Analysis\n",
        "\n",
        "Let's create some visualizations to better understand the data structure and any patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Create a figure with multiple subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('ICICI Bank Statement Data Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Transaction count by type (Debit vs Credit)\n",
        "if 'Debit Amt' in cleaned_df.columns and 'Credit Amt' in cleaned_df.columns:\n",
        "    debit_count = (cleaned_df['Debit Amt'] > 0).sum()\n",
        "    credit_count = (cleaned_df['Credit Amt'] > 0).sum()\n",
        "    \n",
        "    axes[0, 0].pie([debit_count, credit_count], \n",
        "                    labels=['Debit Transactions', 'Credit Transactions'],\n",
        "                    autopct='%1.1f%%', startangle=90)\n",
        "    axes[0, 0].set_title('Transaction Distribution')\n",
        "\n",
        "# Plot 2: Amount distribution (if numeric data available)\n",
        "if 'Debit Amt' in cleaned_df.columns:\n",
        "    debit_amounts = cleaned_df['Debit Amt'].dropna()\n",
        "    if len(debit_amounts) > 0:\n",
        "        axes[0, 1].hist(debit_amounts, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
        "        axes[0, 1].set_title('Debit Amount Distribution')\n",
        "        axes[0, 1].set_xlabel('Amount')\n",
        "        axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# Plot 3: Balance over time (if date and balance available)\n",
        "if 'Balance' in cleaned_df.columns:\n",
        "    balance_data = cleaned_df['Balance'].dropna()\n",
        "    if len(balance_data) > 0:\n",
        "        axes[1, 0].plot(range(len(balance_data)), balance_data, marker='o', linewidth=2, markersize=4)\n",
        "        axes[1, 0].set_title('Balance Over Transactions')\n",
        "        axes[1, 0].set_xlabel('Transaction Number')\n",
        "        axes[1, 0].set_ylabel('Balance')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Data completeness\n",
        "if len(cleaned_df.columns) > 0:\n",
        "    completeness = cleaned_df.notna().sum() / len(cleaned_df) * 100\n",
        "    axes[1, 1].bar(completeness.index, completeness.values, color='skyblue', edgecolor='black')\n",
        "    axes[1, 1].set_title('Data Completeness by Column')\n",
        "    axes[1, 1].set_ylabel('Completeness (%)')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add percentage labels on bars\n",
        "    for i, v in enumerate(completeness.values):\n",
        "        axes[1, 1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\ud83d\udcca Visual analysis completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Insights\n",
        "\n",
        "Let's summarize what we've learned and provide insights about the data extraction process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83d\udccb ICICI Bank Statement Analysis Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n\ud83d\udcc4 Input PDF: {pdf_path}\")\n",
        "print(f\"\ud83d\udcca Expected Output: {result_csv_path}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d Extraction Results:\")\n",
        "print(f\"  \u2022 Total transactions extracted: {len(extracted_df)}\")\n",
        "print(f\"  \u2022 Transactions after cleaning: {len(cleaned_df)}\")\n",
        "print(f\"  \u2022 Expected transactions: {len(expected_df)}\")\n",
        "\n",
        "print(f\"\\n\u2705 Schema Validation: {'PASSED' if schema_valid else 'FAILED'}\")\n",
        "\n",
        "if schema_valid:\n",
        "    print(f\"\\n\ud83d\udcca Data Quality Metrics:\")\n",
        "    for col in cleaned_df.columns:\n",
        "        completeness = cleaned_df[col].notna().sum() / len(cleaned_df) * 100\n",
        "        print(f\"  \u2022 {col}: {completeness:.1f}% complete\")\n",
        "    \n",
        "    print(f\"\\n\ud83c\udfaf Key Insights:\")\n",
        "    print(f\"  \u2022 PDF structure successfully parsed\")\n",
        "    print(f\"  \u2022 Transaction data accurately extracted\")\n",
        "    print(f\"  \u2022 Data format matches expected schema\")\n",
        "    print(f\"  \u2022 Ready for automated processing\")\n",
        "else:\n",
        "    print(f\"\\n\u26a0\ufe0f  Areas for Improvement:\")\n",
        "    print(f\"  \u2022 Schema mismatch needs resolution\")\n",
        "    print(f\"  \u2022 Data extraction logic may need refinement\")\n",
        "    print(f\"  \u2022 Column mapping requires adjustment\")\n",
        "\n",
        "print(f\"\\n\ud83d\ude80 Next Steps:\")\n",
        "print(f\"  \u2022 Integrate with AI agent workflow\")\n",
        "print(f\"  \u2022 Implement automated testing\")\n",
        "print(f\"  \u2022 Scale to other bank statement formats\")\n",
        "print(f\"  \u2022 Deploy as production service\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Testing Our Parser\n",
        "\n",
        "Finally, let's test our custom parser to see how it performs compared to our manual extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test our custom parser\n",
        "try:\n",
        "    import sys\n",
        "    sys.path.append('.')\n",
        "    \n",
        "    from custom_parsers.icici_parser import parse\n",
        "    \n",
        "    print(\"\ud83e\uddea Testing Custom Parser...\")\n",
        "    \n",
        "    # Parse using our custom parser\n",
        "    parser_result = parse(pdf_path)\n",
        "    \n",
        "    print(f\"\u2705 Parser executed successfully!\")\n",
        "    print(f\"\ud83d\udcca Result shape: {parser_result.shape}\")\n",
        "    print(f\"\ud83d\udccb Result columns: {list(parser_result.columns)}\")\n",
        "    \n",
        "    print(\"\\n\ud83d\udccb Parser Output:\")\n",
        "    display(parser_result.head())\n",
        "    \n",
        "    # Compare with expected output\n",
        "    if parser_result.shape == expected_df.shape:\n",
        "        print(\"\\n\u2705 Parser output matches expected dimensions!\")\n",
        "    else:\n",
        "        print(f\"\\n\u26a0\ufe0f  Dimension mismatch: Expected {expected_df.shape}, Got {parser_result.shape}\")\n",
        "        \n",
        "except ImportError as e:\n",
        "    print(f\"\u274c Could not import custom parser: {e}\")\n",
        "    print(\"\ud83d\udca1 Make sure the custom_parsers directory is in your Python path\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Parser execution failed: {e}\")\n",
        "    print(\"\ud83d\udca1 Check the parser implementation for errors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 Conclusion\n",
        "\n",
        "This notebook demonstrates the complete workflow of processing ICICI bank statements:\n",
        "\n",
        "1. **PDF Analysis** - Understanding document structure\n",
        "2. **Data Extraction** - Using pdfplumber to extract tables\n",
        "3. **Data Processing** - Cleaning and formatting raw data\n",
        "4. **Schema Validation** - Ensuring output matches expected format\n",
        "5. **Data Comparison** - Comparing extracted vs expected results\n",
        "6. **Visual Analysis** - Understanding data patterns and quality\n",
        "7. **Parser Testing** - Validating our automated solution\n",
        "\n",
        "The workflow successfully demonstrates how our AI agent can:\n",
        "- Analyze PDF structure\n",
        "- Extract relevant transaction data\n",
        "- Process and clean the data\n",
        "- Validate against expected schemas\n",
        "- Generate working parsers\n",
        "\n",
        "This foundation enables automated processing of various bank statement formats, making financial data extraction scalable and reliable."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}