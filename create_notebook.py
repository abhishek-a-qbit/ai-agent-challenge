#!/usr/bin/env python3
"""
Script to create the ICICI Statement Analysis Demo Jupyter notebook
"""

import json

notebook_content = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ICICI Bank Statement Analysis Demo\n",
                "\n",
                "This notebook demonstrates the complete workflow of processing an ICICI bank statement PDF, extracting transaction data, and comparing it with the expected results.\n",
                "\n",
                "## What we'll cover:\n",
                "1. **PDF Analysis** - Understanding the structure of the ICICI statement\n",
                "2. **Data Extraction** - Using pdfplumber to extract tables and text\n",
                "3. **Data Processing** - Converting raw data to structured format\n",
                "4. **Schema Validation** - Ensuring our output matches the expected CSV structure\n",
                "5. **Comparison** - Comparing extracted data with the reference `result.csv`\n",
                "\n",
                "## Files used:\n",
                "- `data/icici/icici sample.pdf` - Input bank statement\n",
                "- `data/icici/result.csv` - Expected output format\n",
                "- `custom_parsers/icici_parser.py` - Our custom parser implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import pdfplumber\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set display options for better readability\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_rows', 50)\n",
                "pd.set_option('display.width', None)\n",
                "pd.set_option('display.max_colwidth', None)\n",
                "\n",
                "print(\"✅ Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Understanding the Expected Output Schema\n",
                "\n",
                "First, let's examine the `result.csv` file to understand the expected structure and data format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the expected result CSV\n",
                "result_csv_path = \"data/icici/result.csv\"\n",
                "expected_df = pd.read_csv(result_csv_path)\n",
                "\n",
                "print(\"📊 Expected Output Schema:\")\n",
                "print(f\"Shape: {expected_df.shape}\")\n",
                "print(f\"Columns: {list(expected_df.columns)}\")\n",
                "print(\"\\n📋 Data Types:\")\n",
                "print(expected_df.dtypes)\n",
                "print(\"\\n🔍 First few rows:\")\n",
                "display(expected_df.head())\n",
                "print(\"\\n📈 Summary statistics:\")\n",
                "display(expected_df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Analyzing the PDF Structure\n",
                "\n",
                "Now let's examine the ICICI sample PDF to understand its structure and identify where the transaction data is located."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Open and analyze the PDF\n",
                "pdf_path = \"data/icici/icici sample.pdf\"\n",
                "\n",
                "with pdfplumber.open(pdf_path) as pdf:\n",
                "    print(f\"📄 PDF Analysis Results:\")\n",
                "    print(f\"Total pages: {len(pdf.pages)}\")\n",
                "    \n",
                "    # Analyze each page\n",
                "    for page_num, page in enumerate(pdf.pages):\n",
                "        print(f\"\\n📖 Page {page_num + 1}:\")\n",
                "        \n",
                "        # Extract text\n",
                "        text = page.extract_text()\n",
                "        if text:\n",
                "            print(f\"Text length: {len(text)} characters\")\n",
                "            print(f\"First 200 chars: {text[:200]}...\")\n",
                "        \n",
                "        # Extract tables\n",
                "        tables = page.extract_tables()\n",
                "        print(f\"Tables found: {len(tables)}\")\n",
                "        \n",
                "        # Show table structures\n",
                "        for table_idx, table in enumerate(tables):\n",
                "            if table:\n",
                "                print(f\"  Table {table_idx + 1}: {len(table)} rows x {len(table[0]) if table[0] else 0} columns\")\n",
                "                if len(table) > 0:\n",
                "                    print(f\"    First row: {table[0]}\")\n",
                "                    if len(table) > 1:\n",
                "                        print(f\"    Second row: {table[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Extracting Transaction Data\n",
                "\n",
                "Let's extract the transaction data from the PDF tables and see what we get."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract all tables from the PDF\n",
                "with pdfplumber.open(pdf_path) as pdf:\n",
                "    all_tables = []\n",
                "    \n",
                "    for page in pdf.pages:\n",
                "        tables = page.extract_tables()\n",
                "        all_tables.extend(tables)\n",
                "    \n",
                "    print(f\"📊 Total tables extracted: {len(all_tables)}\")\n",
                "    \n",
                "    # Display each table\n",
                "    for i, table in enumerate(all_tables):\n",
                "        if table and len(table) > 0:\n",
                "            print(f\"\\n📋 Table {i + 1}:\")\n",
                "            print(f\"Shape: {len(table)} rows x {len(table[0]) if table[0] else 0} columns\")\n",
                "            \n",
                "            # Convert to DataFrame for better display\n",
                "            df = pd.DataFrame(table)\n",
                "            if len(df) > 0:\n",
                "                display(df.head(10))  # Show first 10 rows\n",
                "                \n",
                "                # Check if this looks like a transaction table\n",
                "                if len(df.columns) >= 4:  # At least Date, Description, Debit, Credit columns\n",
                "                    print(f\"  🎯 Potential transaction table detected!\")\n",
                "                    print(f\"  📅 Columns: {list(df.iloc[0]) if len(df) > 0 else 'No data'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Implementing the Parser Logic\n",
                "\n",
                "Now let's implement the logic to extract and process transaction data, similar to what our agent would generate."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_transactions_from_pdf(pdf_path):\n",
                "    \"\"\"\n",
                "    Extract transaction data from ICICI bank statement PDF.\n",
                "    This function implements the logic that our AI agent would generate.\n",
                "    \"\"\"\n",
                "    transactions = []\n",
                "    \n",
                "    with pdfplumber.open(pdf_path) as pdf:\n",
                "        for page in pdf.pages:\n",
                "            tables = page.extract_tables()\n",
                "            \n",
                "            for table in tables:\n",
                "                if not table or len(table) < 2:\n",
                "                    continue\n",
                "                    \n",
                "                # Look for transaction tables (should have multiple columns)\n",
                "                if len(table[0]) >= 4:\n",
                "                    # Check if first row looks like headers\n",
                "                    first_row = table[0]\n",
                "                    \n",
                "                    # Process each row (skip header)\n",
                "                    for row in table[1:]:\n",
                "                        if len(row) >= 4 and any(cell and str(cell).strip() for cell in row):\n",
                "                            # Extract transaction data\n",
                "                            transaction = {\n",
                "                                'Date': row[0] if len(row) > 0 else None,\n",
                "                                'Description': row[1] if len(row) > 1 else None,\n",
                "                                'Debit Amt': row[2] if len(row) > 2 else None,\n",
                "                                'Credit Amt': row[3] if len(row) > 3 else None,\n",
                "                                'Balance': row[4] if len(row) > 4 else None\n",
                "                            }\n",
                "                            \n",
                "                            # Clean up the data\n",
                "                            for key, value in transaction.items():\n",
                "                                if value is not None:\n",
                "                                    # Remove extra whitespace\n",
                "                                    if isinstance(value, str):\n",
                "                                        value = value.strip()\n",
                "                                    # Convert empty strings to None\n",
                "                                    if value == '':\n",
                "                                        value = None\n",
                "                                    transaction[key] = value\n",
                "                            \n",
                "                            # Only add if we have meaningful data\n",
                "                            if transaction['Date'] and transaction['Description']:\n",
                "                                transactions.append(transaction)\n",
                "    \n",
                "    return transactions\n",
                "\n",
                "# Extract transactions\n",
                "print(\"🔍 Extracting transactions from PDF...\")\n",
                "extracted_transactions = extract_transactions_from_pdf(pdf_path)\n",
                "print(f\"✅ Extracted {len(extracted_transactions)} transactions\")\n",
                "\n",
                "# Convert to DataFrame\n",
                "extracted_df = pd.DataFrame(extracted_transactions)\n",
                "print(\"\\n📊 Extracted Data:\")\n",
                "display(extracted_df.head(10))\n",
                "print(f\"\\n📈 Shape: {extracted_df.shape}\")\n",
                "print(f\"📋 Columns: {list(extracted_df.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Cleaning and Formatting\n",
                "\n",
                "Now let's clean and format the extracted data to match the expected schema."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_and_format_transactions(df):\n",
                "    \"\"\"\n",
                "    Clean and format the extracted transaction data to match expected schema.\n",
                "    \"\"\"\n",
                "    # Create a copy to avoid modifying original\n",
                "    cleaned_df = df.copy()\n",
                "    \n",
                "    # Convert numeric columns\n",
                "    numeric_columns = ['Debit Amt', 'Credit Amt', 'Balance']\n",
                "    \n",
                "    for col in numeric_columns:\n",
                "        if col in cleaned_df.columns:\n",
                "            # Remove currency symbols and commas\n",
                "            cleaned_df[col] = cleaned_df[col].astype(str).str.replace(r'[₹,₹\\s]', '', regex=True)\n",
                "            # Convert to numeric, errors='coerce' will set invalid values to NaN\n",
                "            cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')\n",
                "    \n",
                "    # Clean date column\n",
                "    if 'Date' in cleaned_df.columns:\n",
                "        # Remove any extra whitespace\n",
                "        cleaned_df['Date'] = cleaned_df['Date'].astype(str).str.strip()\n",
                "        # Convert empty strings to None\n",
                "        cleaned_df['Date'] = cleaned_df['Date'].replace('', None)\n",
                "    \n",
                "    # Clean description column\n",
                "    if 'Description' in cleaned_df.columns:\n",
                "        cleaned_df['Description'] = cleaned_df['Description'].astype(str).str.strip()\n",
                "        cleaned_df['Description'] = cleaned_df['Description'].replace('', None)\n",
                "    \n",
                "    # Remove rows where both Date and Description are None\n",
                "    cleaned_df = cleaned_df.dropna(subset=['Date', 'Description'], how='all')\n",
                "    \n",
                "    return cleaned_df\n",
                "\n",
                "# Clean the extracted data\n",
                "print(\"🧹 Cleaning and formatting extracted data...\")\n",
                "cleaned_df = clean_and_format_transactions(extracted_df)\n",
                "\n",
                "print(\"\\n📊 Cleaned Data:\")\n",
                "display(cleaned_df.head(10))\n",
                "print(f\"\\n📈 Shape after cleaning: {cleaned_df.shape}\")\n",
                "print(\"\\n🔍 Data types:\")\n",
                "print(cleaned_df.dtypes)\n",
                "print(\"\\n📊 Summary statistics:\")\n",
                "display(cleaned_df.describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Schema Validation\n",
                "\n",
                "Let's validate that our extracted data matches the expected schema from `result.csv`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "def validate_schema(extracted_df, expected_df):\n",
                "    \"\"\"\n",
                "    Validate that the extracted data matches the expected schema.\n",
                "    \"\"\"\n",
                "    print(\"🔍 Schema Validation Results:\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Check column names\n",
                "    expected_columns = list(expected_df.columns)\n",
                "    extracted_columns = list(extracted_df.columns)\n",
                "    \n",
                "    print(f\"📋 Expected columns: {expected_columns}\")\n",
                "    print(f\"📋 Extracted columns: {extracted_columns}\")\n",
                "    \n",
                "    # Check if all expected columns are present\n",
                "    missing_columns = set(expected_columns) - set(extracted_columns)\n",
                "    extra_columns = set(extracted_columns) - set(expected_columns)\n",
                "    \n",
                "    if missing_columns:\n",
                "        print(f\"❌ Missing columns: {missing_columns}\")\n",
                "    else:\n",
                "        print(\"✅ All expected columns are present\")\n",
                "        \n",
                "    if extra_columns:\n",
                "        print(f\"⚠️  Extra columns: {extra_columns}\")\n",
                "    else:\n",
                "        print(\"✅ No extra columns\")\n",
                "    \n",
                "    # Check data types\n",
                "    print(\"\\n🔍 Data Type Comparison:\")\n",
                "    for col in expected_columns:\n",
                "        if col in extracted_df.columns:\n",
                "            expected_type = expected_df[col].dtype\n",
                "            extracted_type = extracted_df[col].dtype\n",
                "            print(f\"  {col}: Expected {expected_type}, Got {extracted_type}\")\n",
                "        \n",
                "    # Check row count\n",
                "    print(f\"\\n📊 Row Count Comparison:\")\n",
                "    print(f\"  Expected: {len(expected_df)} rows\")\n",
                "    print(f\"  Extracted: {len(extracted_df)} rows\")\n",
                "    \n",
                "    return len(missing_columns) == 0\n",
                "\n",
                "# Validate schema\n",
                "schema_valid = validate_schema(cleaned_df, expected_df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Data Comparison and Analysis\n",
                "\n",
                "Now let's compare our extracted data with the expected results and analyze any differences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Ensure both DataFrames have the same columns for comparison\n",
                "if schema_valid:\n",
                "    # Reorder extracted columns to match expected order\n",
                "    cleaned_df = cleaned_df[expected_df.columns]\n",
                "    \n",
                "    print(\"📊 Data Comparison:\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # Display both DataFrames side by side\n",
                "    print(\"\\n📋 Expected Data (result.csv):\")\n",
                "    display(expected_df.head())\n",
                "    \n",
                "    print(\"\\n📋 Extracted Data (from PDF):\")\n",
                "    display(cleaned_df.head())\n",
                "    \n",
                "    # Check for exact matches\n",
                "    print(\"\\n🔍 Checking for exact matches...\")\n",
                "    \n",
                "    # Reset indices for comparison\n",
                "    expected_reset = expected_df.reset_index(drop=True)\n",
                "    extracted_reset = cleaned_df.reset_index(drop=True)\n",
                "    \n",
                "    # Compare DataFrames\n",
                "    if expected_reset.equals(extracted_reset):\n",
                "        print(\"✅ Perfect match! Extracted data exactly matches expected results.\")\n",
                "    else:\n",
                "        print(\"⚠️  Data doesn't match exactly. Let's analyze the differences...\")\n",
                "        \n",
                "        # Show differences\n",
                "        print(\"\\n📊 Shape comparison:\")\n",
                "        print(f\"Expected: {expected_reset.shape}\")\n",
                "        print(f\"Extracted: {extracted_reset.shape}\")\n",
                "        \n",
                "        # Compare first few rows in detail\n",
                "        print(\"\\n🔍 Detailed comparison of first few rows:\")\n",
                "        for i in range(min(3, len(expected_reset), len(extracted_reset))):\n",
                "            print(f\"\\nRow {i}:\")\n",
                "            print(f\"  Expected: {dict(expected_reset.iloc[i])}\")\n",
                "            print(f\"  Extracted: {dict(extracted_reset.iloc[i])}\")\n",
                "            \n",
                "            # Check if this row matches\n",
                "            if expected_reset.iloc[i].equals(extracted_reset.iloc[i]):\n",
                "                print(\"  ✅ Row matches\")\n",
                "            else:\n",
                "                print(\"  ❌ Row doesn't match\")\n",
                "else:\n",
                "    print(\"❌ Schema validation failed. Cannot compare data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visual Analysis\n",
                "\n",
                "Let's create some visualizations to better understand the data structure and any patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set up the plotting style\n",
                "plt.style.use('default')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "# Create a figure with multiple subplots\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
                "fig.suptitle('ICICI Bank Statement Data Analysis', fontsize=16, fontweight='bold')\n",
                "\n",
                "# Plot 1: Transaction count by type (Debit vs Credit)\n",
                "if 'Debit Amt' in cleaned_df.columns and 'Credit Amt' in cleaned_df.columns:\n",
                "    debit_count = (cleaned_df['Debit Amt'] > 0).sum()\n",
                "    credit_count = (cleaned_df['Credit Amt'] > 0).sum()\n",
                "    \n",
                "    axes[0, 0].pie([debit_count, credit_count], \n",
                "                    labels=['Debit Transactions', 'Credit Transactions'],\n",
                "                    autopct='%1.1f%%', startangle=90)\n",
                "    axes[0, 0].set_title('Transaction Distribution')\n",
                "\n",
                "# Plot 2: Amount distribution (if numeric data available)\n",
                "if 'Debit Amt' in cleaned_df.columns:\n",
                "    debit_amounts = cleaned_df['Debit Amt'].dropna()\n",
                "    if len(debit_amounts) > 0:\n",
                "        axes[0, 1].hist(debit_amounts, bins=20, alpha=0.7, color='red', edgecolor='black')\n",
                "        axes[0, 1].set_title('Debit Amount Distribution')\n",
                "        axes[0, 1].set_xlabel('Amount')\n",
                "        axes[0, 1].set_ylabel('Frequency')\n",
                "\n",
                "# Plot 3: Balance over time (if date and balance available)\n",
                "if 'Balance' in cleaned_df.columns:\n",
                "    balance_data = cleaned_df['Balance'].dropna()\n",
                "    if len(balance_data) > 0:\n",
                "        axes[1, 0].plot(range(len(balance_data)), balance_data, marker='o', linewidth=2, markersize=4)\n",
                "        axes[1, 0].set_title('Balance Over Transactions')\n",
                "        axes[1, 0].set_xlabel('Transaction Number')\n",
                "        axes[1, 0].set_ylabel('Balance')\n",
                "        axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Plot 4: Data completeness\n",
                "if len(cleaned_df.columns) > 0:\n",
                "    completeness = cleaned_df.notna().sum() / len(cleaned_df) * 100\n",
                "    axes[1, 1].bar(completeness.index, completeness.values, color='skyblue', edgecolor='black')\n",
                "    axes[1, 1].set_title('Data Completeness by Column')\n",
                "    axes[1, 1].set_ylabel('Completeness (%)')\n",
                "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
                "    \n",
                "    # Add percentage labels on bars\n",
                "    for i, v in enumerate(completeness.values):\n",
                "        axes[1, 1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"📊 Visual analysis completed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Summary and Insights\n",
                "\n",
                "Let's summarize what we've learned and provide insights about the data extraction process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"📋 ICICI Bank Statement Analysis Summary\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "print(f\"\\n📄 Input PDF: {pdf_path}\")\n",
                "print(f\"📊 Expected Output: {result_csv_path}\")\n",
                "\n",
                "print(f\"\\n🔍 Extraction Results:\")\n",
                "print(f\"  • Total transactions extracted: {len(extracted_df)}\")\n",
                "print(f\"  • Transactions after cleaning: {len(cleaned_df)}\")\n",
                "print(f\"  • Expected transactions: {len(expected_df)}\")\n",
                "\n",
                "print(f\"\\n✅ Schema Validation: {'PASSED' if schema_valid else 'FAILED'}\")\n",
                "\n",
                "if schema_valid:\n",
                "    print(f\"\\n📊 Data Quality Metrics:\")\n",
                "    for col in cleaned_df.columns:\n",
                "        completeness = cleaned_df[col].notna().sum() / len(cleaned_df) * 100\n",
                "        print(f\"  • {col}: {completeness:.1f}% complete\")\n",
                "    \n",
                "    print(f\"\\n🎯 Key Insights:\")\n",
                "    print(f\"  • PDF structure successfully parsed\")\n",
                "    print(f\"  • Transaction data accurately extracted\")\n",
                "    print(f\"  • Data format matches expected schema\")\n",
                "    print(f\"  • Ready for automated processing\")\n",
                "else:\n",
                "    print(f\"\\n⚠️  Areas for Improvement:\")\n",
                "    print(f\"  • Schema mismatch needs resolution\")\n",
                "    print(f\"  • Data extraction logic may need refinement\")\n",
                "    print(f\"  • Column mapping requires adjustment\")\n",
                "\n",
                "print(f\"\\n🚀 Next Steps:\")\n",
                "print(f\"  • Integrate with AI agent workflow\")\n",
                "print(f\"  • Implement automated testing\")\n",
                "print(f\"  • Scale to other bank statement formats\")\n",
                "print(f\"  • Deploy as production service\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Testing Our Parser\n",
                "\n",
                "Finally, let's test our custom parser to see how it performs compared to our manual extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test our custom parser\n",
                "try:\n",
                "    import sys\n",
                "    sys.path.append('.')\n",
                "    \n",
                "    from custom_parsers.icici_parser import parse\n",
                "    \n",
                "    print(\"🧪 Testing Custom Parser...\")\n",
                "    \n",
                "    # Parse using our custom parser\n",
                "    parser_result = parse(pdf_path)\n",
                "    \n",
                "    print(f\"✅ Parser executed successfully!\")\n",
                "    print(f\"📊 Result shape: {parser_result.shape}\")\n",
                "    print(f\"📋 Result columns: {list(parser_result.columns)}\")\n",
                "    \n",
                "    print(\"\\n📋 Parser Output:\")\n",
                "    display(parser_result.head())\n",
                "    \n",
                "    # Compare with expected output\n",
                "    if parser_result.shape == expected_df.shape:\n",
                "        print(\"\\n✅ Parser output matches expected dimensions!\")\n",
                "    else:\n",
                "        print(f\"\\n⚠️  Dimension mismatch: Expected {expected_df.shape}, Got {parser_result.shape}\")\n",
                "        \n",
                "except ImportError as e:\n",
                "    print(f\"❌ Could not import custom parser: {e}\")\n",
                "    print(\"💡 Make sure the custom_parsers directory is in your Python path\")\n",
                "except Exception as e:\n",
                "    print(f\"❌ Parser execution failed: {e}\")\n",
                "    print(\"💡 Check the parser implementation for errors\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 🎉 Conclusion\n",
                "\n",
                "This notebook demonstrates the complete workflow of processing ICICI bank statements:\n",
                "\n",
                "1. **PDF Analysis** - Understanding document structure\n",
                "2. **Data Extraction** - Using pdfplumber to extract tables\n",
                "3. **Data Processing** - Cleaning and formatting raw data\n",
                "4. **Schema Validation** - Ensuring output matches expected format\n",
                "5. **Data Comparison** - Comparing extracted vs expected results\n",
                "6. **Visual Analysis** - Understanding data patterns and quality\n",
                "7. **Parser Testing** - Validating our automated solution\n",
                "\n",
                "The workflow successfully demonstrates how our AI agent can:\n",
                "- Analyze PDF structure\n",
                "- Extract relevant transaction data\n",
                "- Process and clean the data\n",
                "- Validate against expected schemas\n",
                "- Generate working parsers\n",
                "\n",
                "This foundation enables automated processing of various bank statement formats, making financial data extraction scalable and reliable."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}

# Write the notebook to file
with open('ICICI_Statement_Analysis_Demo.ipynb', 'w') as f:
    json.dump(notebook_content, f, indent=2)

print("✅ Jupyter notebook created successfully!")
print("📁 File: ICICI_Statement_Analysis_Demo.ipynb")
print("🚀 You can now open this file in Jupyter Notebook or JupyterLab") 